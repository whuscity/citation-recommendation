{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始文件读取\n",
    "df = pd.read_csv(\"D:/dblp_full.csv\",usecols = ['id','author_id','references','year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取作者、关键词、引文和文章id信息\n",
    "authors = list(df['author_id'])\n",
    "keywords = list(df['new_keywords'])\n",
    "refs = list(df['references'])\n",
    "ids = list(df['id'].astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 统计作者、论文和关键词信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不重复的论文数为103424\n",
      "不重复的作者数为137438\n",
      "不重复的关键词数为248357\n"
     ]
    }
   ],
   "source": [
    "id_df = list(df['id'].astype(str))\n",
    "author_df = list(df['author_id'])\n",
    "keyword_df = list(df['new_keywords'])\n",
    "aus = []\n",
    "kws = []\n",
    "print(\"不重复的论文数为{}\".format(len(set(id_df))))\n",
    "for au in author_df:\n",
    "    if type(au)!=float and len(au)>0:\n",
    "        aus += au.split(\";\")\n",
    "print(\"不重复的作者数为{}\".format(len(set(aus))))\n",
    "for kw in keyword_df:\n",
    "    if type(kw)!=float and len(kw)>0:\n",
    "        kws += kw.split(\";\")\n",
    "print(\"不重复的关键词数为{}\".format(len(set(kws))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获取node_type文件 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(103424):\n",
    "    p.append(str(i)+\" \"+\"P\")\n",
    "for j in range(103424,240862):\n",
    "    a.append(str(j)+\" \"+\"A\")\n",
    "for m in range(240862,489219):\n",
    "    k.append(str(m)+\" \"+\"K\")\n",
    "total = p+a+k\n",
    "with open(\"D:/dblp_node_type.txt\",\"w\") as f:\n",
    "    for each in total:\n",
    "        f.write(each+\"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作者 关键词 论文ID化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_refs_ids(x,paper_ids):\n",
    "    refs = []\n",
    "    if type(x)!=float and len(x)>0:\n",
    "        for ref in x.split(\";\"):\n",
    "            refs.append(str(paper_ids.index(str(ref))))\n",
    "    return \";\".join(refs)\n",
    "\n",
    "def get_paper_id(x,paper_ids):\n",
    "    x = str(paper_ids.index(str(x)))\n",
    "    return x\n",
    "def get_author_id(x,author_ids):\n",
    "    aus_x = []\n",
    "    if type(x)!=float and len(x)>0:\n",
    "        \n",
    "        for au in x.split(\";\"):\n",
    "            aus_x.append(str(author_ids.index(au)+103424))\n",
    "    return \";\".join(aus_x)\n",
    "def get_keyword_id(x,keyword_ids):\n",
    "    kws_x = []\n",
    "    if type(x)!=float and len(x)>0:\n",
    "        \n",
    "        for kw in x.split(\";\"):\n",
    "            kws_x.append(str(keyword_ids.index(kw)+240862))\n",
    "    return \";\".join(kws_x)\n",
    "paper_ids = list(set(id_df))\n",
    "author_ids = list(set(aus))\n",
    "keyword_ids = list(set(kws))\n",
    "df['paper_id'] = df['id'].apply(get_paper_id,paper_ids = paper_ids)\n",
    "print(\"文章id化结束\")\n",
    "df['author_id'] = df['author_id'].apply(get_author_id,author_ids = author_ids)\n",
    "print(\"作者id化结束\")\n",
    "df['refs_id'] = df['references'].apply(get_refs_ids,paper_ids = paper_ids)\n",
    "print(\"引文id化结束\")\n",
    "df['keyword_id'] = df['new_keywords'].apply(get_keyword_id,keyword_ids = keyword_ids)\n",
    "print(\"关键词id化结束\")\n",
    "df.to_csv(\"D:/dblp.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 候选文献数据集/目标文献数据集划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7295\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7149"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orgin = df[df['year']>2011]\n",
    "print(len(orgin))\n",
    "orgin_ids = list(orgin['paper_id'].astype(str))\n",
    "orgin_refs = list(orgin['refs_id'])\n",
    "useful = []\n",
    "for x,y in zip(orgin_ids,orgin_refs):\n",
    "    if type(y)!=float and len(y)>0:\n",
    "        useful.append(x)\n",
    "train_df = df[df['paper_id'].isin(useful)]\n",
    "paper_IDS = list(df['paper_id'].astype(str))\n",
    "test = [each for each in paper_IDS if each not in useful]\n",
    "test_df = df[df['paper_id'].isin(test)]\n",
    "train_df.to_csv(\"D:/dblp_train.csv\",index=False)\n",
    "test_df.to_csv(\"D:/dblp_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获取训练集/测试集的边关系：论文-关键字/作者-论文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.txt  2014年前的边关系 pd.concat(axis=0)\n",
    "#valid.txt 2014年后的边关系，只包括文章与文章的边关系\n",
    "#features.txt 包括所有节点的特征关系\n",
    "#node_type.txt 节点边的类型\n",
    "# 1：作者-撰写-论文\n",
    "# 2：论文-包含-关键词\n",
    "# 3：论文-引用-论文\n",
    "# valid.txt增加三种关系的负例\n",
    "# test.txt增加三种关系的负例\n",
    "# 用阿拉伯数字表示1-16707表示为论文编号16708-35568表示为作者编号  35569-44057表示为关键词编号\n",
    "def get_author_paper_edges(authors,paper):\n",
    "    a_p_edges = []\n",
    "    normal_a_p_edges = []\n",
    "    for i,author in enumerate(authors):\n",
    "        if type(author)!= float and len(author)>0:\n",
    "            for au in author.split(\";\"):\n",
    "                a_p_edges.append(\"1 \"+ au + \" \" + paper[i])\n",
    "                normal_a_p_edges.append(au + \" \" + paper[i])\n",
    "    return a_p_edges, normal_a_p_edges\n",
    "def get_paper_keyword_edges(papers,keywords):\n",
    "    p_k_edges = []\n",
    "    normal_p_k_edges = []\n",
    "    for i,keyword in enumerate(keywords):\n",
    "        if type(keyword)!= float and len(keyword)>0:\n",
    "            for kw in keyword.split(\";\"):\n",
    "                p_k_edges.append(\"2 \" + papers[i] + \" \" + kw )\n",
    "                normal_p_k_edges .append(papers[i] + \" \"+ kw)\n",
    "    return p_k_edges, normal_p_k_edges\n",
    "def get_paper_paper_edges(papers,refs):\n",
    "    p_p_edges = []\n",
    "    normal_p_p_edges = []\n",
    "    for i,ref in enumerate(refs):\n",
    "        if type(ref)!= float and len(ref)>0:\n",
    "            for r in ref.split(\";\"):\n",
    "                p_p_edges.append(\"3 \" + papers[i] + \" \" + r )\n",
    "                normal_p_p_edges .append(papers[i] + \" \"+ r)\n",
    "    return p_p_edges, normal_p_p_edges     \n",
    "def write_text(texts,filepath):\n",
    "    with open(filepath,\"w\",encoding = \"utf-8\") as f:\n",
    "        for each in texts:\n",
    "            f.write(each+\"\\n\")\n",
    "        f.close()\n",
    "train_paper = list(map(lambda x: str(x),list(train_df['paper_id'])))\n",
    "train_author = list(train_df['author_id'])\n",
    "train_keywords = list(train_df['keyword_id'])\n",
    "train_refs =  list(train_df['refs_id'])\n",
    "train_a_p_edges,normal_train_a_p_edges = get_author_paper_edges(train_author,train_paper)\n",
    "train_p_k_edges,normal_p_k_edges =  get_paper_keyword_edges(train_paper,train_keywords)\n",
    "train_p_p_edges,normal_train_p_p_edges = get_paper_paper_edges(train_paper,train_refs)\n",
    "gatne_train_edges = train_a_p_edges + train_p_k_edges + train_p_p_edges\n",
    "normal_train_edges = normal_train_a_p_edges + normal_p_k_edges + normal_train_p_p_edges\n",
    "write_text(gatne_train_edges,\"D:/datasets/dblp_gatne_train.txt\")\n",
    "write_text(normal_train_edges,\"D:/datasets/dblp_normal_train.txt\")\n",
    "train_paper_ids = list(map(lambda x: str(x),list(train_df['paper_id'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目标文献数据集正确引文获取及目标文献数据集更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "test_paper_ids = list(map(lambda x: str(x),list(test_df['paper_id'])))\n",
    "test_refs = list(test_df['refs_id'])\n",
    "true_cites={}\n",
    "for x,y in zip(test_paper_ids,test_refs):\n",
    "    new_true_cites = []\n",
    "    if type(y)!=float and len(y)>0:\n",
    "         for each in y.split(\";\"):\n",
    "            if each in train_paper_ids:\n",
    "                    new_true_cites.append(each)\n",
    "         if len(new_true_cites)==0:\n",
    "            continue\n",
    "         else:\n",
    "            true_cites[x] = new_true_cites\n",
    "json.dump(true_cites,open(\"D:/datasets/dblp_true_cites.json\",\"w\")) \n",
    "last_true_ids = list(true_cites.keys())\n",
    "last_test_df = test_df[test_df['paper_id'].astype(str).isin(last_true_ids)]\n",
    "last_test_df.to_csv(\"D:/datasets/dblp_test.csv\",index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
